Описание данных, скачать их вы можете по [ссылке](https://yadi.sk/d/K-BvJ8WJJeQM5g):
- CropNumbers.rar 
    Это наш основной датасет. В нем картинки вырезанных номеров машин,
    а таргет хранится в названии картинки.  
- FullPlate.rar 
    Это дополнительный датасет, про который я расскажу на след паре.

__Внимание! Изменять файлы сделанные на предыдущем этапе можно и даже НУЖНО!__


__Учить и предиктить в докер контейнере!__

Третий этап: 
- Выполнить все `#TODO` в файлах папки `worker`.

- У вас есть два стрима `visualize_stream.py` и `ocr_stream.py`. Все стримы связывает **state**, в нем вы можете определить буффер для фреймов.
- В `ocr_stream.py` нужно добавить предиктор и предсказывать на изображении, и последующую обработку текста (если необходимо, перенос на цпу, например) и результат закинуть в **state**.
- В `visualize_stream.py`из необходимо брать кадр и на нем отрисовывать тот текст который вы предсказали. 
- В `run.py` рекомендуется заменить вызов `sys.argv` либо на парсер, либо на конфиг. 


Есть так же один важный момент в подходе реализации коммуникации между стримами. 
Первый: можно выполнять последовательно ```ocr -> visualize -> video_writer``` -- долгий, можно асинхронно выполнять `ocr` и `visualize`, тогда в зависимости от скорости работы отдельных частей они могут рассинхронироваться, но занимать времени в ~2 раза меньше. Обычно предпочтительнее второй вариант и оптимизировать работу всех частей, чтоб получить максимальный прирост в скорости. Модель должна работать риал-тайм в 30 fps.

__Учить и предиктить в докер контейнере!__

Второй этап: 

- Предварительно рекомендую разбить выборку на трейн и валидацию.
- Желательно создать себе репозиторий для проекта и пользоваться гитом. 
- В файле `train_script.py` необходимо определить датасет, 
задать применяемые трансформы, которые вы написали на предыдущем этапе.
Обратите внимание, что `runner` catalyst-а принимает на вход словарь. 
Это обновление появилось в файле `dataset.py`, но если вы просто вставили свой 
скрипт.
- Необходимо написать `predictor.py`, который может принимать на вход
как батч, так и 1 картинку из потока и возвращать текст.
- Также желательно добавить accuracy в `metrics.py`. Видео по [CTCLoss](https://youtu.be/eYIL4TMAeRI).

После этого вы должны смочь запустить обучение. 
На этапе обучения вам пригодятся следующие трюки: 
- Аугментации делают модель устойчивей к шумам. Помните она 
должна будет работать на реальном видосе!
- Размер картинки можно увеличивать, но модель будет становится 
тяжелее. Она должна будет работать на видео минимум в 30 fps.
При финальной оценке проекта скорость будет учитываться. 
- Существует множество разных Callback-ов. Для разных стратегий 
изменения лернинг рейта, для сохранения модели, для измерения метрик,
 EarlyStopping - для того, что максимально выставленное число эпох не досчитывалось, в случае
 если модель не становится лучше и т.д.
=======


